# llama.cpp 性能优化记录

## 当前配置（已实现）
- **模型**: Qwen3-1.7B
- **量化**: q4_K_M
- **线程数**: 16（匹配 CPU 核心数）
- **批处理线程**: 16
- **上下文大小**: 2048
- **批处理大小**: 512
- **内存锁定**: 启用（--mlock）

## 性能对比
| 配置 | 推理时间 | 平均/题 | 准确率 |
|------|---------|---------|--------|
| 8线程 | 102.42s | 1.93s | 88.68% |
| 16线程 + 优化 | 93.50s | 1.76s | 86.79% |

## 进一步优化建议

### 1. 量化策略
- **q4_0**: 更快，但准确率可能略低
- **q5_K_M**: 更准确，但速度略慢
- **q8_0**: 接近原始精度，但模型更大

### 2. 并行推理（如果任务允许）
当前是串行处理每个问题。如果评估允许，可以考虑：
- 批量推理（llama.cpp 的 parallel 参数）
- 多进程并行（启动多个 llama-cli 实例）

### 3. CPU 优化
- 确保使用 AVX2 版本的 llama-cli（已实现）
- 考虑编译 AVX512 版本（如果目标 CPU 支持）
- 禁用 CPU 频率缩放（需要系统权限）

### 4. 内存优化
- `--mlock`: 已启用，防止模型被换出到 swap
- 预加载模型到内存（warmup）：当前禁用以节省时间

### 5. 提示词优化
- 更简洁的 system prompt
- 减少不必要的上下文
- 使用更直接的指令

### 6. llama.cpp 参数调优
```bash
-t 16          # 主线程数
-tb 16         # 批处理线程数
-c 2048        # 上下文窗口（当前）
-b 512         # 批处理大小（当前）
--mlock        # 锁定内存（当前）
-ngl 0         # GPU 层数（CPU only，保持 0）
--numa         # NUMA 优化（如果是多 socket CPU）
```

## CPU 利用率问题分析

### 为什么 CPU 利用率波动？

1. **Token 生成的串行特性**
   - LLM 推理是自回归的：每个 token 依赖前一个
   - 无法完全并行化生成过程
   - 导致 CPU 利用率在不同阶段波动

2. **推理阶段差异**
   - **Prompt 处理阶段**: 可以高度并行，CPU 利用率高
   - **Token 生成阶段**: 串行性强，CPU 利用率相对较低
   - **内存带宽瓶颈**: 大模型推理受限于内存带宽而非计算

3. **与 PyTorch 的对比**
   - PyTorch 可能使用更激进的线程策略
   - llama.cpp 更注重效率和内存使用
   - 不同的并行策略导致不同的 CPU 使用模式

### 改进方向

1. **如果追求最高 CPU 利用率**：
   - 启动多个并行的 llama-cli 进程处理不同问题
   - 但要注意内存限制（每个进程都需要加载模型）

2. **如果追求最快推理速度**：
   - 当前配置已经接近最优
   - 主要瓶颈在内存带宽，而非 CPU 计算能力

3. **平衡方案**（推荐）：
   - 保持当前单进程配置
   - 使用 16 线程充分利用 CPU
   - 接受 CPU 利用率的自然波动（这是 LLM 推理的特性）

## 结论

当前配置已经针对 16 核 CPU 进行了优化：
- ✅ 使用所有 16 个核心
- ✅ 启用内存锁定防止 swap
- ✅ 优化批处理参数
- ✅ 使用 AVX2 指令集

CPU 利用率的波动是 LLM 自回归推理的固有特性，不代表性能问题。
实际推理速度（1.76s/问题）已经很好，在 2 小时限制内可以处理 ~4000 个问题。

